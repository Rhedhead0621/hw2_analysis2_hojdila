{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff5b59-794e-42fb-b025-bad33c8e67f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "689dc5f9-80b9-44ec-b4ac-7d251b0e8e1f",
   "metadata": {},
   "source": [
    "# Working with CSV data files\n",
    "\n",
    "Tabular data is often saved in some sort of CSV like format. Sometimes the delimiter might be a tab or some other character instead of a comma. By convention, CSV files have a `.csv` extension (tab delimited files will often have a `.tsv` extension). CSV is such a common format that Python has a built in `csv` library. In addition, other Python libraries designed for data analysis (such as **pandas** or **NumPy**) often have their own functions for working with CSV files. In some cases, these functions are simply \"wrappers\" around Python's built in CSV related functions.\n",
    "\n",
    "There isn't an actual standard format for CSV files. It's more accurate to say that there are various dialects of CSV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce6820-cbfd-4056-befd-6c26926f5603",
   "metadata": {},
   "source": [
    "## Read and write `kc_house_data.csv` with pandas\n",
    "\n",
    "We will start with a file of housing price data available from Kaggle at https://www.kaggle.com/datasets/harlfoxem/housesalesprediction. The file is named `kc_house_data.csv` and it's available in the `data` folder. \n",
    "\n",
    "Here's a little snippet of the file opened in a text editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c70313-688f-42b9-8268-f68fe2c67fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'images/kc_house_data_snippet.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m----> 2\u001b[0m Image(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages/kc_house_data_snippet.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aap\\Lib\\site-packages\\IPython\\core\\display.py:970\u001b[0m, in \u001b[0;36mImage.__init__\u001b[1;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munconfined \u001b[38;5;241m=\u001b[39m unconfined\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malt \u001b[38;5;241m=\u001b[39m alt\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28msuper\u001b[39m(Image, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(data\u001b[38;5;241m=\u001b[39mdata, url\u001b[38;5;241m=\u001b[39murl, filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m    971\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata)\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, {}):\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aap\\Lib\\site-packages\\IPython\\core\\display.py:327\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[1;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreload()\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aap\\Lib\\site-packages\\IPython\\core\\display.py:1005\u001b[0m, in \u001b[0;36mImage.reload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m-> 1005\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Image,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mreload()\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretina:\n\u001b[0;32m   1007\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retina_shape()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aap\\Lib\\site-packages\\IPython\\core\\display.py:353\u001b[0m, in \u001b[0;36mDisplayObject.reload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_flags \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 353\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_flags, encoding\u001b[38;5;241m=\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;66;03m# Deferred import\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'images/kc_house_data_snippet.png'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/kc_house_data_snippet.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b104dbd9-8b0d-4877-90be-9fe2039f20f6",
   "metadata": {},
   "source": [
    "The file is pretty typical for CSV files in that:\n",
    "\n",
    "- it contains a header line to be used as column names\n",
    "- each row contains a number of values separated by commas (CSV = comma separated values)\n",
    "- some of the values in each row are numeric while others are \"text\" and are enclosed in double quotes\n",
    "- the text columns are intended to be read into some non-numeric data type such as a string\n",
    "\n",
    "A few other things to note:\n",
    "\n",
    "- the `id` column, while it looks numeric isn't something we are going to do math with. It's just an identifier and the double quotes around it just reinforce the fact that we should interpret it as a label of some sort.\n",
    "- the `date` column is also non-numeric with a `'T'` separating the date and time. Depending on what we are using this data for, we may have to try to convert this datetime-like string into a true Python datetime value.\n",
    "- the zip code is like the `id` column, it looks numeric but it's really a label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbecbf4-a479-466d-b422-482c03c573d5",
   "metadata": {},
   "source": [
    "Often we just want to read a CSV file into a pandas dataframe. For this we can use the pandas `read_csv()` function. It has a huge number of input arguments for customizing how you want the file read. Let's start with everything at the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8523f076-862d-44d3-9282-c47461b59c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f39e9e77-31f2-4654-ab3f-4130adf17a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21613 entries, 0 to 21612\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             21613 non-null  int64  \n",
      " 1   date           21613 non-null  object \n",
      " 2   price          21613 non-null  float64\n",
      " 3   bedrooms       21613 non-null  int64  \n",
      " 4   bathrooms      21613 non-null  float64\n",
      " 5   sqft_living    21613 non-null  int64  \n",
      " 6   sqft_lot       21613 non-null  int64  \n",
      " 7   floors         21613 non-null  float64\n",
      " 8   waterfront     21613 non-null  int64  \n",
      " 9   view           21613 non-null  int64  \n",
      " 10  condition      21613 non-null  int64  \n",
      " 11  grade          21613 non-null  int64  \n",
      " 12  sqft_above     21613 non-null  int64  \n",
      " 13  sqft_basement  21613 non-null  int64  \n",
      " 14  yr_built       21613 non-null  int64  \n",
      " 15  yr_renovated   21613 non-null  int64  \n",
      " 16  zipcode        21613 non-null  int64  \n",
      " 17  lat            21613 non-null  float64\n",
      " 18  long           21613 non-null  float64\n",
      " 19  sqft_living15  21613 non-null  int64  \n",
      " 20  sqft_lot15     21613 non-null  int64  \n",
      "dtypes: float64(5), int64(15), object(1)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "kc1 = pd.read_csv('data/kc_house_data.csv')\n",
    "kc1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb315415-6f5a-4dec-9b13-416aa9ee84f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0           0     0  ...      7        1180              0   \n",
       "1      7242     2.0           0     0  ...      7        2170            400   \n",
       "2     10000     1.0           0     0  ...      6         770              0   \n",
       "3      5000     1.0           0     0  ...      7        1050            910   \n",
       "4      8080     1.0           0     0  ...      8        1680              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "3      1965             0    98136  47.5208 -122.393           1360   \n",
       "4      1987             0    98074  47.6168 -122.045           1800   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "1        7639  \n",
       "2        8062  \n",
       "3        5000  \n",
       "4        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kc1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31765660-67d6-40fd-9665-7ebe60350726",
   "metadata": {},
   "source": [
    "It seems that:\n",
    "\n",
    "- pandas ignored the quotations marks and made its best guess as to the appropriate data type for those columns.\n",
    "- `id` ended up as `int64` while `floors` is a `float64` (some \"split-level\" houses might have 1.5 floors)\n",
    "- the `date` column got interpreted as a string `object`\n",
    "\n",
    "Before pandas 1.0, all strings got stored as an `object`, but now there is a `StringDtype` which is recommended for string columns. See https://pandas.pydata.org/docs/user_guide/text.html#text-types.\n",
    "\n",
    "While we could fix up some of these datatypes once the CSV file has been read into a `DataFrame`, let's see what we can do with `read_csv` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f14f1a0-14f9-4a3c-b6a4-f29bb163978c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msep\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None | lib.NoDefault'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdelimiter\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None | lib.NoDefault'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mheader\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"int | Sequence[int] | None | Literal['infer']\"\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnames\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Sequence[Hashable] | None | lib.NoDefault'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mindex_col\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'IndexLabel | Literal[False] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0musecols\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'UsecolsArgType'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'DtypeArg | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'CSVEngine | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mconverters\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Mapping[Hashable, Callable] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtrue_values\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'list | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfalse_values\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'list | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mskipinitialspace\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mskiprows\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'list[int] | int | Callable[[Hashable], bool] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mskipfooter\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnrows\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mna_values\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Hashable | Iterable[Hashable] | Mapping[Hashable, Iterable[Hashable]] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mkeep_default_na\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mna_filter\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool | lib.NoDefault'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mskip_blank_lines\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mparse_dates\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool | Sequence[Hashable] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0minfer_datetime_format\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool | lib.NoDefault'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mkeep_date_col\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool | lib.NoDefault'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdate_parser\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Callable | lib.NoDefault'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdate_format\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | dict[Hashable, str] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdayfirst\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcache_dates\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mchunksize\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'CompressionOptions'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mthousands\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdecimal\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlineterminator\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mquotechar\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\"'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mquoting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdoublequote\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mescapechar\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcomment\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mencoding_errors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'strict'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdialect\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | csv.Dialect | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mon_bad_lines\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdelim_whitespace\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool | lib.NoDefault'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmemory_map\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfloat_precision\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Literal['high', 'legacy'] | None\"\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'StorageOptions | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdtype_backend\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'DtypeBackend | lib.NoDefault'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'DataFrame | TextFileReader'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Read a comma-separated values (csv) file into DataFrame.\n",
       "\n",
       "Also supports optionally iterating or breaking of the file\n",
       "into chunks.\n",
       "\n",
       "Additional help can be found in the online docs for\n",
       "`IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "filepath_or_buffer : str, path object or file-like object\n",
       "    Any valid string path is acceptable. The string could be a URL. Valid\n",
       "    URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
       "    expected. A local file could be: file://localhost/path/to/table.csv.\n",
       "\n",
       "    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
       "\n",
       "    By file-like object, we refer to objects with a ``read()`` method, such as\n",
       "    a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
       "sep : str, default ','\n",
       "    Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n",
       "    C engine cannot automatically detect\n",
       "    the separator, but the Python parsing engine can, meaning the latter will\n",
       "    be used and automatically detect the separator from only the first valid\n",
       "    row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n",
       "    In addition, separators longer than 1 character and different from\n",
       "    ``'\\s+'`` will be interpreted as regular expressions and will also force\n",
       "    the use of the Python parsing engine. Note that regex delimiters are prone\n",
       "    to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
       "delimiter : str, optional\n",
       "    Alias for ``sep``.\n",
       "header : int, Sequence of int, 'infer' or None, default 'infer'\n",
       "    Row number(s) containing column labels and marking the start of the\n",
       "    data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n",
       "    are passed the behavior is identical to ``header=0`` and column\n",
       "    names are inferred from the first line of the file, if column\n",
       "    names are passed explicitly to ``names`` then the behavior is identical to\n",
       "    ``header=None``. Explicitly pass ``header=0`` to be able to\n",
       "    replace existing names. The header can be a list of integers that\n",
       "    specify row locations for a :class:`~pandas.MultiIndex` on the columns\n",
       "    e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n",
       "    skipped (e.g. 2 in this example is skipped). Note that this\n",
       "    parameter ignores commented lines and empty lines if\n",
       "    ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
       "    data rather than the first line of the file.\n",
       "names : Sequence of Hashable, optional\n",
       "    Sequence of column labels to apply. If the file contains a header row,\n",
       "    then you should explicitly pass ``header=0`` to override the column names.\n",
       "    Duplicates in this list are not allowed.\n",
       "index_col : Hashable, Sequence of Hashable or False, optional\n",
       "  Column(s) to use as row label(s), denoted either by column labels or column\n",
       "  indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n",
       "  will be formed for the row labels.\n",
       "\n",
       "  Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
       "  column as the index, e.g., when you have a malformed file with delimiters at\n",
       "  the end of each line.\n",
       "usecols : Sequence of Hashable or Callable, optional\n",
       "    Subset of columns to select, denoted either by column labels or column indices.\n",
       "    If list-like, all elements must either\n",
       "    be positional (i.e. integer indices into the document columns) or strings\n",
       "    that correspond to column names provided either by the user in ``names`` or\n",
       "    inferred from the document header row(s). If ``names`` are given, the document\n",
       "    header row(s) are not taken into account. For example, a valid list-like\n",
       "    ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
       "    Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
       "    To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n",
       "    preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n",
       "    for columns in ``['foo', 'bar']`` order or\n",
       "    ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
       "    for ``['bar', 'foo']`` order.\n",
       "\n",
       "    If callable, the callable function will be evaluated against the column\n",
       "    names, returning names where the callable function evaluates to ``True``. An\n",
       "    example of a valid callable argument would be ``lambda x: x.upper() in\n",
       "    ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
       "    parsing time and lower memory usage.\n",
       "dtype : dtype or dict of {Hashable : dtype}, optional\n",
       "    Data type(s) to apply to either the whole dataset or individual columns.\n",
       "    E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n",
       "    Use ``str`` or ``object`` together with suitable ``na_values`` settings\n",
       "    to preserve and not interpret ``dtype``.\n",
       "    If ``converters`` are specified, they will be applied INSTEAD\n",
       "    of ``dtype`` conversion.\n",
       "\n",
       "    .. versionadded:: 1.5.0\n",
       "\n",
       "        Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n",
       "        the default determines the ``dtype`` of the columns which are not explicitly\n",
       "        listed.\n",
       "engine : {'c', 'python', 'pyarrow'}, optional\n",
       "    Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
       "    is currently more feature-complete. Multithreading is currently only supported by\n",
       "    the pyarrow engine.\n",
       "\n",
       "    .. versionadded:: 1.4.0\n",
       "\n",
       "        The 'pyarrow' engine was added as an *experimental* engine, and some features\n",
       "        are unsupported, or may not work correctly, with this engine.\n",
       "converters : dict of {Hashable : Callable}, optional\n",
       "    Functions for converting values in specified columns. Keys can either\n",
       "    be column labels or column indices.\n",
       "true_values : list, optional\n",
       "    Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\n",
       "false_values : list, optional\n",
       "    Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\n",
       "skipinitialspace : bool, default False\n",
       "    Skip spaces after delimiter.\n",
       "skiprows : int, list of int or Callable, optional\n",
       "    Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n",
       "    at the start of the file.\n",
       "\n",
       "    If callable, the callable function will be evaluated against the row\n",
       "    indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n",
       "    An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
       "skipfooter : int, default 0\n",
       "    Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\n",
       "nrows : int, optional\n",
       "    Number of rows of file to read. Useful for reading pieces of large files.\n",
       "na_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n",
       "    Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n",
       "    per-column ``NA`` values.  By default the following values are interpreted as\n",
       "    ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n",
       "    \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n",
       "    \"n/a\", \"nan\", \"null \".\n",
       "\n",
       "keep_default_na : bool, default True\n",
       "    Whether or not to include the default ``NaN`` values when parsing the data.\n",
       "    Depending on whether ``na_values`` is passed in, the behavior is as follows:\n",
       "\n",
       "    * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n",
       "      is appended to the default ``NaN`` values used for parsing.\n",
       "    * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n",
       "      the default ``NaN`` values are used for parsing.\n",
       "    * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n",
       "      the ``NaN`` values specified ``na_values`` are used for parsing.\n",
       "    * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n",
       "      strings will be parsed as ``NaN``.\n",
       "\n",
       "    Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n",
       "    ``na_values`` parameters will be ignored.\n",
       "na_filter : bool, default True\n",
       "    Detect missing value markers (empty strings and the value of ``na_values``). In\n",
       "    data without any ``NA`` values, passing ``na_filter=False`` can improve the\n",
       "    performance of reading a large file.\n",
       "verbose : bool, default False\n",
       "    Indicate number of ``NA`` values placed in non-numeric columns.\n",
       "\n",
       "    .. deprecated:: 2.2.0\n",
       "skip_blank_lines : bool, default True\n",
       "    If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\n",
       "parse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n",
       "    The behavior is as follows:\n",
       "\n",
       "    * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n",
       "      ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n",
       "    * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n",
       "      each as a separate date column.\n",
       "    * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n",
       "      as a single date column. Values are joined with a space before parsing.\n",
       "    * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n",
       "      result 'foo'. Values are joined with a space before parsing.\n",
       "\n",
       "    If a column or index cannot be represented as an array of ``datetime``,\n",
       "    say because of an unparsable value or a mixture of timezones, the column\n",
       "    or index will be returned unaltered as an ``object`` data type. For\n",
       "    non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n",
       "    :func:`~pandas.read_csv`.\n",
       "\n",
       "    Note: A fast-path exists for iso8601-formatted dates.\n",
       "infer_datetime_format : bool, default False\n",
       "    If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n",
       "    format of the ``datetime`` strings in the columns, and if it can be inferred,\n",
       "    switch to a faster method of parsing them. In some cases this can increase\n",
       "    the parsing speed by 5-10x.\n",
       "\n",
       "    .. deprecated:: 2.0.0\n",
       "        A strict version of this argument is now the default, passing it has no effect.\n",
       "\n",
       "keep_date_col : bool, default False\n",
       "    If ``True`` and ``parse_dates`` specifies combining multiple columns then\n",
       "    keep the original columns.\n",
       "date_parser : Callable, optional\n",
       "    Function to use for converting a sequence of string columns to an array of\n",
       "    ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n",
       "    conversion. pandas will try to call ``date_parser`` in three different ways,\n",
       "    advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
       "    (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n",
       "    string values from the columns defined by ``parse_dates`` into a single array\n",
       "    and pass that; and 3) call ``date_parser`` once for each row using one or\n",
       "    more strings (corresponding to the columns defined by ``parse_dates``) as\n",
       "    arguments.\n",
       "\n",
       "    .. deprecated:: 2.0.0\n",
       "       Use ``date_format`` instead, or read in as ``object`` and then apply\n",
       "       :func:`~pandas.to_datetime` as-needed.\n",
       "date_format : str or dict of column -> format, optional\n",
       "    Format to use for parsing dates when used in conjunction with ``parse_dates``.\n",
       "    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n",
       "    `strftime documentation\n",
       "    <https://docs.python.org/3/library/datetime.html\n",
       "    #strftime-and-strptime-behavior>`_ for more information on choices, though\n",
       "    note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n",
       "    You can also pass:\n",
       "\n",
       "    - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n",
       "        time string (not necessarily in exactly the same format);\n",
       "    - \"mixed\", to infer the format for each element individually. This is risky,\n",
       "        and you should probably use it along with `dayfirst`.\n",
       "\n",
       "    .. versionadded:: 2.0.0\n",
       "dayfirst : bool, default False\n",
       "    DD/MM format dates, international and European format.\n",
       "cache_dates : bool, default True\n",
       "    If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n",
       "    conversion. May produce significant speed-up when parsing duplicate\n",
       "    date strings, especially ones with timezone offsets.\n",
       "\n",
       "iterator : bool, default False\n",
       "    Return ``TextFileReader`` object for iteration or getting chunks with\n",
       "    ``get_chunk()``.\n",
       "chunksize : int, optional\n",
       "    Number of lines to read from the file per chunk. Passing a value will cause the\n",
       "    function to return a ``TextFileReader`` object for iteration.\n",
       "    See the `IO Tools docs\n",
       "    <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
       "    for more information on ``iterator`` and ``chunksize``.\n",
       "\n",
       "compression : str or dict, default 'infer'\n",
       "    For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n",
       "    path-like, then detect compression from the following extensions: '.gz',\n",
       "    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
       "    (otherwise no compression).\n",
       "    If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n",
       "    Set to ``None`` for no decompression.\n",
       "    Can also be a dict with key ``'method'`` set\n",
       "    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
       "    other key-value pairs are forwarded to\n",
       "    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
       "    ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n",
       "    ``tarfile.TarFile``, respectively.\n",
       "    As an example, the following could be passed for Zstandard decompression using a\n",
       "    custom compression dictionary:\n",
       "    ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
       "\n",
       "    .. versionadded:: 1.5.0\n",
       "        Added support for `.tar` files.\n",
       "\n",
       "    .. versionchanged:: 1.4.0 Zstandard support.\n",
       "\n",
       "thousands : str (length 1), optional\n",
       "    Character acting as the thousands separator in numerical values.\n",
       "decimal : str (length 1), default '.'\n",
       "    Character to recognize as decimal point (e.g., use ',' for European data).\n",
       "lineterminator : str (length 1), optional\n",
       "    Character used to denote a line break. Only valid with C parser.\n",
       "quotechar : str (length 1), optional\n",
       "    Character used to denote the start and end of a quoted item. Quoted\n",
       "    items can include the ``delimiter`` and it will be ignored.\n",
       "quoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n",
       "    Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n",
       "    ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n",
       "    characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n",
       "    or ``lineterminator``.\n",
       "doublequote : bool, default True\n",
       "   When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n",
       "   whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n",
       "   field as a single ``quotechar`` element.\n",
       "escapechar : str (length 1), optional\n",
       "    Character used to escape other characters.\n",
       "comment : str (length 1), optional\n",
       "    Character indicating that the remainder of line should not be parsed.\n",
       "    If found at the beginning\n",
       "    of a line, the line will be ignored altogether. This parameter must be a\n",
       "    single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
       "    fully commented lines are ignored by the parameter ``header`` but not by\n",
       "    ``skiprows``. For example, if ``comment='#'``, parsing\n",
       "    ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n",
       "    treated as the header.\n",
       "encoding : str, optional, default 'utf-8'\n",
       "    Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n",
       "    standard encodings\n",
       "    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
       "\n",
       "encoding_errors : str, optional, default 'strict'\n",
       "    How encoding errors are treated. `List of possible values\n",
       "    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
       "\n",
       "    .. versionadded:: 1.3.0\n",
       "\n",
       "dialect : str or csv.Dialect, optional\n",
       "    If provided, this parameter will override values (default or not) for the\n",
       "    following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n",
       "    ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n",
       "    override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n",
       "    documentation for more details.\n",
       "on_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n",
       "    Specifies what to do upon encountering a bad line (a line with too many fields).\n",
       "    Allowed values are :\n",
       "\n",
       "    - ``'error'``, raise an Exception when a bad line is encountered.\n",
       "    - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n",
       "    - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n",
       "\n",
       "    .. versionadded:: 1.3.0\n",
       "\n",
       "    .. versionadded:: 1.4.0\n",
       "\n",
       "        - Callable, function with signature\n",
       "          ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n",
       "          bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
       "          If the function returns ``None``, the bad line will be ignored.\n",
       "          If the function returns a new ``list`` of strings with more elements than\n",
       "          expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
       "          Only supported when ``engine='python'``\n",
       "\n",
       "    .. versionchanged:: 2.2.0\n",
       "\n",
       "        - Callable, function with signature\n",
       "          as described in `pyarrow documentation\n",
       "          <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n",
       "          #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n",
       "\n",
       "delim_whitespace : bool, default False\n",
       "    Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n",
       "    used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n",
       "    is set to ``True``, nothing should be passed in for the ``delimiter``\n",
       "    parameter.\n",
       "\n",
       "    .. deprecated:: 2.2.0\n",
       "        Use ``sep=\"\\s+\"`` instead.\n",
       "low_memory : bool, default True\n",
       "    Internally process the file in chunks, resulting in lower memory use\n",
       "    while parsing, but possibly mixed type inference.  To ensure no mixed\n",
       "    types either set ``False``, or specify the type with the ``dtype`` parameter.\n",
       "    Note that the entire file is read into a single :class:`~pandas.DataFrame`\n",
       "    regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n",
       "    chunks. (Only valid with C parser).\n",
       "memory_map : bool, default False\n",
       "    If a filepath is provided for ``filepath_or_buffer``, map the file object\n",
       "    directly onto memory and access the data directly from there. Using this\n",
       "    option can improve performance because there is no longer any I/O overhead.\n",
       "float_precision : {'high', 'legacy', 'round_trip'}, optional\n",
       "    Specifies which converter the C engine should use for floating-point\n",
       "    values. The options are ``None`` or ``'high'`` for the ordinary converter,\n",
       "    ``'legacy'`` for the original lower precision pandas converter, and\n",
       "    ``'round_trip'`` for the round-trip converter.\n",
       "\n",
       "storage_options : dict, optional\n",
       "    Extra options that make sense for a particular storage connection, e.g.\n",
       "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
       "    are forwarded to ``urllib.request.Request`` as header options. For other\n",
       "    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
       "    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
       "    details, and for more examples on storage options refer `here\n",
       "    <https://pandas.pydata.org/docs/user_guide/io.html?\n",
       "    highlight=storage_options#reading-writing-remote-files>`_.\n",
       "\n",
       "dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n",
       "    Back-end data type applied to the resultant :class:`DataFrame`\n",
       "    (still experimental). Behaviour is as follows:\n",
       "\n",
       "    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n",
       "      (default).\n",
       "    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n",
       "      DataFrame.\n",
       "\n",
       "    .. versionadded:: 2.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       "DataFrame or TextFileReader\n",
       "    A comma-separated values (csv) file is returned as two-dimensional\n",
       "    data structure with labeled axes.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
       "read_table : Read general delimited file into DataFrame.\n",
       "read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\ahojd\\anaconda3\\envs\\aap\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fb9761-fda8-4f1c-b1bc-3dd0839371fa",
   "metadata": {},
   "source": [
    "Wow, there are a ton of options for customizing the behavior of `read_csv()` to deal with many common problems associated with ingesting text files:\n",
    "\n",
    "- delimiter isn't always a comma\n",
    "- skipping some lines at top of the file\n",
    "- parsing dates\n",
    "- handling commas used as a thousands separator or decimal point\n",
    "- only importing a subset of the column and/or rows\n",
    "- dealing with missing data\n",
    "- modifyng data types\n",
    "- transforming values during import\n",
    "- dealing with quotation marks with string fields\n",
    "- dealing with bad lines\n",
    "- ... and many more. \n",
    "\n",
    "For the `id` column, we could move it into the index by using the `index_col=0` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7870dca4-3f87-4bc4-b71b-777c83e174de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 21613 entries, 7129300520 to 1523300157\n",
      "Data columns (total 20 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           21613 non-null  object \n",
      " 1   price          21613 non-null  float64\n",
      " 2   bedrooms       21613 non-null  int64  \n",
      " 3   bathrooms      21613 non-null  float64\n",
      " 4   sqft_living    21613 non-null  int64  \n",
      " 5   sqft_lot       21613 non-null  int64  \n",
      " 6   floors         21613 non-null  float64\n",
      " 7   waterfront     21613 non-null  int64  \n",
      " 8   view           21613 non-null  int64  \n",
      " 9   condition      21613 non-null  int64  \n",
      " 10  grade          21613 non-null  int64  \n",
      " 11  sqft_above     21613 non-null  int64  \n",
      " 12  sqft_basement  21613 non-null  int64  \n",
      " 13  yr_built       21613 non-null  int64  \n",
      " 14  yr_renovated   21613 non-null  int64  \n",
      " 15  zipcode        21613 non-null  int64  \n",
      " 16  lat            21613 non-null  float64\n",
      " 17  long           21613 non-null  float64\n",
      " 18  sqft_living15  21613 non-null  int64  \n",
      " 19  sqft_lot15     21613 non-null  int64  \n",
      "dtypes: float64(5), int64(14), object(1)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "kc2a = pd.read_csv('data/kc_house_data.csv', index_col=0)\n",
    "kc2a.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7979b084-778f-46d9-a861-469cdb48bb8c",
   "metadata": {},
   "source": [
    "If we don't want `id` to be the index, we could specify its datatype using the `dtype` argument. We can pass in a dictionary that specifies the column name and its intended datatype. While we are at it, we can make `zipcode` a string as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30710371-911f-4910-baa5-33d60089effd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21613 entries, 0 to 21612\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             21613 non-null  string \n",
      " 1   date           21613 non-null  object \n",
      " 2   price          21613 non-null  float64\n",
      " 3   bedrooms       21613 non-null  int64  \n",
      " 4   bathrooms      21613 non-null  float64\n",
      " 5   sqft_living    21613 non-null  int64  \n",
      " 6   sqft_lot       21613 non-null  int64  \n",
      " 7   floors         21613 non-null  float64\n",
      " 8   waterfront     21613 non-null  int64  \n",
      " 9   view           21613 non-null  int64  \n",
      " 10  condition      21613 non-null  int64  \n",
      " 11  grade          21613 non-null  int64  \n",
      " 12  sqft_above     21613 non-null  int64  \n",
      " 13  sqft_basement  21613 non-null  int64  \n",
      " 14  yr_built       21613 non-null  int64  \n",
      " 15  yr_renovated   21613 non-null  int64  \n",
      " 16  zipcode        21613 non-null  string \n",
      " 17  lat            21613 non-null  float64\n",
      " 18  long           21613 non-null  float64\n",
      " 19  sqft_living15  21613 non-null  int64  \n",
      " 20  sqft_lot15     21613 non-null  int64  \n",
      "dtypes: float64(5), int64(13), object(1), string(2)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "kc2b = pd.read_csv('data/kc_house_data.csv', dtype={'id': 'string', 'zipcode': 'string'})\n",
    "kc2b.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7983f7d-dcad-47b2-895c-bc9233995f97",
   "metadata": {},
   "source": [
    "We can use the `parse_dates` argument to convert `date` from a string to an actual pandas `datetime64` column. If pandas can't parse the date properly using its default date parser (`dateutil.parser.parser`) we can specify a custom date parser function. \n",
    "\n",
    "The `usecols` argument lets us specify a subset of the columns in the dataframe to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bb2fcc6-dbd3-48b6-bdab-3694c17c4f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21613 entries, 0 to 21612\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   id           21613 non-null  string        \n",
      " 1   date         21613 non-null  datetime64[ns]\n",
      " 2   price        21613 non-null  float64       \n",
      " 3   sqft_living  21613 non-null  int64         \n",
      " 4   zipcode      21613 non-null  string        \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), string(2)\n",
      "memory usage: 844.4 KB\n"
     ]
    }
   ],
   "source": [
    "kc3 = pd.read_csv('data/kc_house_data.csv', \n",
    "                  dtype={'id': 'string', 'zipcode': 'string'}, \n",
    "                  parse_dates=['date'],\n",
    "                  usecols=['id', 'date', 'price', 'sqft_living', 'zipcode'])\n",
    "kc3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "697a1020-5207-47b2-b6f2-c2e27a2591c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>2014-10-13</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>1180</td>\n",
       "      <td>98178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>2014-12-09</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>2570</td>\n",
       "      <td>98125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>770</td>\n",
       "      <td>98028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>2014-12-09</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>1960</td>\n",
       "      <td>98136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>1680</td>\n",
       "      <td>98074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       date     price  sqft_living zipcode\n",
       "0  7129300520 2014-10-13  221900.0         1180   98178\n",
       "1  6414100192 2014-12-09  538000.0         2570   98125\n",
       "2  5631500400 2015-02-25  180000.0          770   98028\n",
       "3  2487200875 2014-12-09  604000.0         1960   98136\n",
       "4  1954400510 2015-02-18  510000.0         1680   98074"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kc3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8eb08-c983-4a7f-bbc3-3034b70f922b",
   "metadata": {},
   "source": [
    "Now let's write out `kc3` to a new CSV file using `to_csv`, which is a `DataFrame` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "871b7ef2-9b86-4b54-94aa-d6676b37dcc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msep\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mna_rep\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfloat_format\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | Callable | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Sequence[Hashable] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mheader\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool_t | list[str]'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool_t'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mindex_label\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'IndexLabel | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'CompressionOptions'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mquoting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mquotechar\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\"'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlineterminator\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mchunksize\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdate_format\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdoublequote\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool_t'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mescapechar\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdecimal\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'OpenFileErrors'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'strict'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'StorageOptions | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'str | None'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Write object to a comma-separated values (csv) file.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "path_or_buf : str, path object, file-like object, or None, default None\n",
       "    String, path object (implementing os.PathLike[str]), or file-like\n",
       "    object implementing a write() function. If None, the result is\n",
       "    returned as a string. If a non-binary file object is passed, it should\n",
       "    be opened with `newline=''`, disabling universal newlines. If a binary\n",
       "    file object is passed, `mode` might need to contain a `'b'`.\n",
       "sep : str, default ','\n",
       "    String of length 1. Field delimiter for the output file.\n",
       "na_rep : str, default ''\n",
       "    Missing data representation.\n",
       "float_format : str, Callable, default None\n",
       "    Format string for floating point numbers. If a Callable is given, it takes\n",
       "    precedence over other numeric formatting parameters, like decimal.\n",
       "columns : sequence, optional\n",
       "    Columns to write.\n",
       "header : bool or list of str, default True\n",
       "    Write out the column names. If a list of strings is given it is\n",
       "    assumed to be aliases for the column names.\n",
       "index : bool, default True\n",
       "    Write row names (index).\n",
       "index_label : str or sequence, or False, default None\n",
       "    Column label for index column(s) if desired. If None is given, and\n",
       "    `header` and `index` are True, then the index names are used. A\n",
       "    sequence should be given if the object uses MultiIndex. If\n",
       "    False do not print fields for index names. Use index_label=False\n",
       "    for easier importing in R.\n",
       "mode : {'w', 'x', 'a'}, default 'w'\n",
       "    Forwarded to either `open(mode=)` or `fsspec.open(mode=)` to control\n",
       "    the file opening. Typical values include:\n",
       "\n",
       "    - 'w', truncate the file first.\n",
       "    - 'x', exclusive creation, failing if the file already exists.\n",
       "    - 'a', append to the end of file if it exists.\n",
       "\n",
       "encoding : str, optional\n",
       "    A string representing the encoding to use in the output file,\n",
       "    defaults to 'utf-8'. `encoding` is not supported if `path_or_buf`\n",
       "    is a non-binary file object.\n",
       "compression : str or dict, default 'infer'\n",
       "    For on-the-fly compression of the output data. If 'infer' and 'path_or_buf' is\n",
       "    path-like, then detect compression from the following extensions: '.gz',\n",
       "    '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
       "    (otherwise no compression).\n",
       "    Set to ``None`` for no compression.\n",
       "    Can also be a dict with key ``'method'`` set\n",
       "    to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
       "    other key-value pairs are forwarded to\n",
       "    ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
       "    ``bz2.BZ2File``, ``zstandard.ZstdCompressor``, ``lzma.LZMAFile`` or\n",
       "    ``tarfile.TarFile``, respectively.\n",
       "    As an example, the following could be passed for faster compression and to create\n",
       "    a reproducible gzip archive:\n",
       "    ``compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}``.\n",
       "\n",
       "    .. versionadded:: 1.5.0\n",
       "        Added support for `.tar` files.\n",
       "\n",
       "       May be a dict with key 'method' as compression mode\n",
       "       and other entries as additional compression options if\n",
       "       compression mode is 'zip'.\n",
       "\n",
       "       Passing compression options as keys in dict is\n",
       "       supported for compression modes 'gzip', 'bz2', 'zstd', and 'zip'.\n",
       "quoting : optional constant from csv module\n",
       "    Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n",
       "    then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n",
       "    will treat them as non-numeric.\n",
       "quotechar : str, default '\\\"'\n",
       "    String of length 1. Character used to quote fields.\n",
       "lineterminator : str, optional\n",
       "    The newline character or character sequence to use in the output\n",
       "    file. Defaults to `os.linesep`, which depends on the OS in which\n",
       "    this method is called ('\\\\n' for linux, '\\\\r\\\\n' for Windows, i.e.).\n",
       "\n",
       "    .. versionchanged:: 1.5.0\n",
       "\n",
       "        Previously was line_terminator, changed for consistency with\n",
       "        read_csv and the standard library 'csv' module.\n",
       "\n",
       "chunksize : int or None\n",
       "    Rows to write at a time.\n",
       "date_format : str, default None\n",
       "    Format string for datetime objects.\n",
       "doublequote : bool, default True\n",
       "    Control quoting of `quotechar` inside a field.\n",
       "escapechar : str, default None\n",
       "    String of length 1. Character used to escape `sep` and `quotechar`\n",
       "    when appropriate.\n",
       "decimal : str, default '.'\n",
       "    Character recognized as decimal separator. E.g. use ',' for\n",
       "    European data.\n",
       "errors : str, default 'strict'\n",
       "    Specifies how encoding and decoding errors are to be handled.\n",
       "    See the errors argument for :func:`open` for a full list\n",
       "    of options.\n",
       "\n",
       "storage_options : dict, optional\n",
       "    Extra options that make sense for a particular storage connection, e.g.\n",
       "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
       "    are forwarded to ``urllib.request.Request`` as header options. For other\n",
       "    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
       "    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
       "    details, and for more examples on storage options refer `here\n",
       "    <https://pandas.pydata.org/docs/user_guide/io.html?\n",
       "    highlight=storage_options#reading-writing-remote-files>`_.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "None or str\n",
       "    If path_or_buf is None, returns the resulting csv format as a\n",
       "    string. Otherwise returns None.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "read_csv : Load a CSV file into a DataFrame.\n",
       "to_excel : Write DataFrame to an Excel file.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Create 'out.csv' containing 'df' without indices\n",
       "\n",
       ">>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n",
       "...                    'mask': ['red', 'purple'],\n",
       "...                    'weapon': ['sai', 'bo staff']})\n",
       ">>> df.to_csv('out.csv', index=False)  # doctest: +SKIP\n",
       "\n",
       "Create 'out.zip' containing 'out.csv'\n",
       "\n",
       ">>> df.to_csv(index=False)\n",
       "'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n",
       ">>> compression_opts = dict(method='zip',\n",
       "...                         archive_name='out.csv')  # doctest: +SKIP\n",
       ">>> df.to_csv('out.zip', index=False,\n",
       "...           compression=compression_opts)  # doctest: +SKIP\n",
       "\n",
       "To write a csv file to a new folder or nested folder you will first\n",
       "need to create it using either Pathlib or os:\n",
       "\n",
       ">>> from pathlib import Path  # doctest: +SKIP\n",
       ">>> filepath = Path('folder/subfolder/out.csv')  # doctest: +SKIP\n",
       ">>> filepath.parent.mkdir(parents=True, exist_ok=True)  # doctest: +SKIP\n",
       ">>> df.to_csv(filepath)  # doctest: +SKIP\n",
       "\n",
       ">>> import os  # doctest: +SKIP\n",
       ">>> os.makedirs('folder/subfolder', exist_ok=True)  # doctest: +SKIP\n",
       ">>> df.to_csv('folder/subfolder/out.csv')  # doctest: +SKIP\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\ahojd\\anaconda3\\envs\\aap\\lib\\site-packages\\pandas\\core\\generic.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame.to_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f69d5-b0f5-4f86-8c3e-d4723e0c1dd8",
   "metadata": {},
   "source": [
    "We'll start with the default behavior. The default behavior is to include the dataframe index in the resulting CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2926da52-5f21-48bb-b41b-dcf452322883",
   "metadata": {},
   "outputs": [],
   "source": [
    "kc3.to_csv('data/kc3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e29360-af9d-4ad8-9fbd-736e001447da",
   "metadata": {},
   "source": [
    "What does the result look like? The default integer index gets included, but there is no corresponding column name in the header row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "312f6e26-60d9-4178-930b-028e33eb0692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",id,date,price,sqft_living,zipcode\n",
      "0,7129300520,2014-10-13,221900.0,1180,98178\n",
      "1,6414100192,2014-12-09,538000.0,2570,98125\n",
      "2,5631500400,2015-02-25,180000.0,770,98028\n",
      "3,2487200875,2014-12-09,604000.0,1960,98136\n"
     ]
    }
   ],
   "source": [
    "with open('data/kc3.csv') as in_kc3:\n",
    "    kc3_lines = in_kc3.readlines()\n",
    "\n",
    "for _ in range(5):\n",
    "    print(kc3_lines[_], end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5fdac-5c4b-4a4a-bcc4-7d06e580f26f",
   "metadata": {},
   "source": [
    "This time the index won't get included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "023d420f-faa5-4c9c-aed8-e04788f204a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,date,price,sqft_living,zipcode\n",
      "7129300520,2014-10-13,221900.0,1180,98178\n",
      "6414100192,2014-12-09,538000.0,2570,98125\n",
      "5631500400,2015-02-25,180000.0,770,98028\n",
      "2487200875,2014-12-09,604000.0,1960,98136\n"
     ]
    }
   ],
   "source": [
    "kc3.to_csv('data/kc3.csv', index=False)\n",
    "\n",
    "with open('data/kc3.csv') as in_kc3:\n",
    "    kc3_lines = in_kc3.readlines()\n",
    "\n",
    "for _ in range(5):\n",
    "    print(kc3_lines[_], end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bace1e-7daa-4c5a-ac6a-8ddf3d8a7e5e",
   "metadata": {},
   "source": [
    "Like `read_csv()`, the `to_csv()` function has a myriad of options for controlling how the dataframe is written to the CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e75a47-9b55-497a-9c1e-f3128d1c166c",
   "metadata": {},
   "source": [
    "## No header line\n",
    "\n",
    "Sometimes a CSV file won't contain a header line. Other times, the header will contain additional lines along with the column names at the top of the file.\n",
    "\n",
    "The file `data/siteA_loc1_shallow.csv` contains stream temperature readings from a sensor placed in a specific stream for several weeks. Each row consists of a datetime, a 'C' or 'F' indicating centigrade or fahrenheit, and the temperature reading. Unfortunately, no header was in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6f694f0-3c26-450f-b74f-6dded5f70bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/2/2009 10:50,C,27\n",
      "9/2/2009 14:50,C,20.5\n",
      "9/2/2009 18:50,C,20.5\n",
      "9/2/2009 22:50,C,20\n",
      "9/3/2009 2:50,C,20\n"
     ]
    }
   ],
   "source": [
    "with open('data/siteA_loc1_shallow.csv') as in_siteA:\n",
    "    siteA_lines = in_siteA.readlines()\n",
    "\n",
    "for _ in range(5):\n",
    "    print(siteA_lines[_], end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8484adf-addb-4cd7-bb3f-feef7a7c024e",
   "metadata": {},
   "source": [
    "This is easily dealt with using the `names` argument for `read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74599b17-fdfa-4f52-bcf6-db4250a36268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>scale</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9/2/2009 10:50</td>\n",
       "      <td>C</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9/2/2009 14:50</td>\n",
       "      <td>C</td>\n",
       "      <td>20.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9/2/2009 18:50</td>\n",
       "      <td>C</td>\n",
       "      <td>20.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9/2/2009 22:50</td>\n",
       "      <td>C</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9/3/2009 2:50</td>\n",
       "      <td>C</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         datetime scale  temperature\n",
       "0  9/2/2009 10:50     C         27.0\n",
       "1  9/2/2009 14:50     C         20.5\n",
       "2  9/2/2009 18:50     C         20.5\n",
       "3  9/2/2009 22:50     C         20.0\n",
       "4   9/3/2009 2:50     C         20.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siteA_df = pd.read_csv('data/siteA_loc1_shallow.csv', names=['datetime','scale','temperature'])\n",
    "siteA_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebd946-8ed5-4b7d-9023-29d9d99a548e",
   "metadata": {},
   "source": [
    "## A more complex header\n",
    "\n",
    "This next example is more complicated. It involves some stream flow data from the [USGS](https://www.usgs.gov/). The data is tab delimited but contains a bunch of comments at the top of the file, followed by the header line, which is then followed by a format specification line. Here's what the first 35 lines of the file look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff597def-7e5b-4e02-b10e-3410dfffd7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ---------------------------------- WARNING ----------------------------------------\n",
      "# Some of the data that you have obtained from this U.S. Geological Survey database\n",
      "# may not have received Director's approval. Any such data values are qualified\n",
      "# as provisional and are subject to revision. Provisional data are released on the\n",
      "# condition that neither the USGS nor the United States Government may be held liable\n",
      "# for any damages resulting from its use.\n",
      "#\n",
      "# Additional info: https://help.waterdata.usgs.gov/policies/provisional-data-statement\n",
      "#\n",
      "# File-format description:  https://help.waterdata.usgs.gov/faq/about-tab-delimited-output\n",
      "# Automated-retrieval info: https://help.waterdata.usgs.gov/faq/automated-retrievals\n",
      "#\n",
      "# Contact:   gs-w_support_nwisweb@usgs.gov\n",
      "# retrieved: 2021-05-27 10:22:40 EDT       (nadww01)\n",
      "#\n",
      "# Data for the following 1 site(s) are contained in this file\n",
      "#    USGS 04161000 CLINTON RIVER AT AUBURN HILLS, MI\n",
      "# -----------------------------------------------------------------------------------\n",
      "#\n",
      "# Data provided for site 04161000\n",
      "#            TS   parameter     Description\n",
      "#        279557       00065     Gage height, feet\n",
      "#        279558       63160     Stream water level elevation above NAVD 1988, in feet\n",
      "#         72218       00060     Discharge, cubic feet per second\n",
      "#\n",
      "# Data-value qualification codes included in this output:\n",
      "#     A  Approved for publication -- Processing and review completed.\n",
      "#     P  Provisional data subject to revision.\n",
      "# \n",
      "agency_cd\tsite_no\tdatetime\ttz_cd\t279557_00065\t279557_00065_cd\t279558_63160\t279558_63160_cd\t72218_00060\t72218_00060_cd\n",
      "5s\t15s\t20d\t6s\t14n\t10s\t14n\t10s\t14n\t10s\n",
      "USGS\t04161000\t2021-01-01 00:00\tEST\t1.59\tA\t847.69\tP\t95.2\tA\n",
      "USGS\t04161000\t2021-01-01 00:15\tEST\t1.60\tA\t847.70\tP\t96.7\tA\n",
      "USGS\t04161000\t2021-01-01 00:30\tEST\t1.60\tA\t847.70\tP\t96.7\tA\n",
      "USGS\t04161000\t2021-01-01 00:45\tEST\t1.61\tA\t847.71\tP\t98.2\tA\n"
     ]
    }
   ],
   "source": [
    "with open('data/clinton_river_hard.csv') as in_crh:\n",
    "    crh_lines = in_crh.readlines()\n",
    "\n",
    "for _ in range(35):\n",
    "    print(crh_lines[_], end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b746f434-8158-4ea0-9324-22970e553f09",
   "metadata": {},
   "source": [
    "The key to getting this data read into a pandas `DataFrame` was careful reading of the pandas documentation for `read_csv`. Here's our challenge:\n",
    "\n",
    "* We need to skip the row immediately after the header row.\n",
    "* The header row itself follows a whole bunch of other rows to be skipped, all of them starting with '#'.\n",
    "\n",
    "So, we can use the `comment='#'` argument to skip the rows above the header. However, in specifiying the row number for the header, pandas does NOT count any comment lines and starts counting at 0. The header line is actually row 0 from the perspective of `read_csv` after the comments are ignored. **However**, to skip the row below the header, we use the `skiprows` argument, and it does **NOT** ignore the comment lines when counting the rows to skip. Pasting part of the file into a text editor reveals that we need to skip row number 30 (again, starting to count at 0). So, let's cheat and hard code in the row number to skip just to show that this works. For the `skiprows` argument we can either pass in a list of row numbers to skip, or, we can pass in a single integer specifying the number of rows to skip. To make it work, we need to do the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9048beb9-acff-4f7e-b8e9-f758a2111de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency_cd</th>\n",
       "      <th>site_no</th>\n",
       "      <th>datetime</th>\n",
       "      <th>tz_cd</th>\n",
       "      <th>279557_00065</th>\n",
       "      <th>279557_00065_cd</th>\n",
       "      <th>279558_63160</th>\n",
       "      <th>279558_63160_cd</th>\n",
       "      <th>72218_00060</th>\n",
       "      <th>72218_00060_cd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USGS</td>\n",
       "      <td>4161000</td>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>1.59</td>\n",
       "      <td>A</td>\n",
       "      <td>847.69</td>\n",
       "      <td>P</td>\n",
       "      <td>95.2</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USGS</td>\n",
       "      <td>4161000</td>\n",
       "      <td>2021-01-01 00:15:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>1.60</td>\n",
       "      <td>A</td>\n",
       "      <td>847.70</td>\n",
       "      <td>P</td>\n",
       "      <td>96.7</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USGS</td>\n",
       "      <td>4161000</td>\n",
       "      <td>2021-01-01 00:30:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>1.60</td>\n",
       "      <td>A</td>\n",
       "      <td>847.70</td>\n",
       "      <td>P</td>\n",
       "      <td>96.7</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USGS</td>\n",
       "      <td>4161000</td>\n",
       "      <td>2021-01-01 00:45:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>1.61</td>\n",
       "      <td>A</td>\n",
       "      <td>847.71</td>\n",
       "      <td>P</td>\n",
       "      <td>98.2</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USGS</td>\n",
       "      <td>4161000</td>\n",
       "      <td>2021-01-01 01:00:00</td>\n",
       "      <td>EST</td>\n",
       "      <td>1.61</td>\n",
       "      <td>A</td>\n",
       "      <td>847.71</td>\n",
       "      <td>P</td>\n",
       "      <td>98.2</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  agency_cd  site_no            datetime tz_cd  279557_00065 279557_00065_cd  \\\n",
       "0      USGS  4161000 2021-01-01 00:00:00   EST          1.59               A   \n",
       "1      USGS  4161000 2021-01-01 00:15:00   EST          1.60               A   \n",
       "2      USGS  4161000 2021-01-01 00:30:00   EST          1.60               A   \n",
       "3      USGS  4161000 2021-01-01 00:45:00   EST          1.61               A   \n",
       "4      USGS  4161000 2021-01-01 01:00:00   EST          1.61               A   \n",
       "\n",
       "   279558_63160 279558_63160_cd  72218_00060 72218_00060_cd  \n",
       "0        847.69               P         95.2              A  \n",
       "1        847.70               P         96.7              A  \n",
       "2        847.70               P         96.7              A  \n",
       "3        847.71               P         98.2              A  \n",
       "4        847.71               P         98.2              A  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flowdata_test = pd.read_csv('data/clinton_river_hard.csv', sep = '\\t', comment='#', \n",
    "                       header = 0, skiprows = [30], parse_dates = ['datetime']) \n",
    "\n",
    "flowdata_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de82710-b4f2-4840-8750-7c701f48336a",
   "metadata": {},
   "source": [
    "And here's what happens if we try telling pandas to skip 1 row after the header. We would end up with the format specification row in our dataframe. So, if the number of comment lines is always the same, then would can use the approach above to skip the row we want. But of course, in real life, the number of comment lines will usually vary and that is certainly the case with this data from the USGA. For example, depending on the type of gauge, a different number of metrics might be captured and that affects the number of rows in the section starting with \"Data provided\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b8a37-7261-4175-b9ae-c91fd83e899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('data/clinton_river_hard.csv', sep = '\\t', comment='#', \n",
    "                       header = 0, skiprows = 1, parse_dates = ['datetime']).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b6b26-483d-4502-8a03-0ee28379ba4d",
   "metadata": {},
   "source": [
    "So, how might we handle this more complex case of a variable number of comment lines and skipping one line after the header? Well, I actually address this problem with this same data source in my [Advanced Analytics with Python course](http://www.sba.oakland.edu/faculty/isken/courses/mis6900/index.html). In addition to this line skipping, I also show to use Python to automatically grab the data from the USGS website and do some datetime manipulations related to time zones. If you are interested you can check out the screencast and associated Jupyter notebook from [this course web page](http://www.sba.oakland.edu/faculty/isken/courses/mis6900/getting_data_from_web.html#web-apis-data-wrangling-with-pandas)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d8973-7629-43fa-940e-ac62921e9539",
   "metadata": {},
   "source": [
    "## Using the `csv` package\n",
    "\n",
    "You may have data in a CSV file that you want to work with in other ways than just reading it into a pandas `DataFrame`. For example, you may have some row by row processing that needs to be done before moving the data to some new file or database. The `csv` package makes it easy to iterate through a CSV file using a `reader` object. \n",
    "\n",
    "Much like pandas, there are various input arguments for controlling the reading process in terms of dealing with things like whitespace, quote characters, and delimiters. \n",
    "In addition, the `csv` package uses the notion of *dialects* to group together commonly used input argument values. The default dialect is named `'excel'` and is designed to handle CSV files generated by Excel. As you might guess, this dialect includes things like using a comma for the delimiter and Windows style line endings. If you want all the details, see this [SO post](https://stackoverflow.com/questions/49204639/what-exactly-are-the-csv-modules-dialect-settings-for-excel-tab).\n",
    "\n",
    "The `csv` package also supports writing CSV files using a `writer` object. \n",
    "\n",
    "Let's see some simple examples. We'll start by just iterating through a CSV file containing station information for a bike sharing system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8bf60-a954-4a8a-99ac-053cf5940a97",
   "metadata": {},
   "source": [
    "### Reading a CSV file with `csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f114f-14f2-4086-8b09-3b6bb47b702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04cfdc8-8049-46d0-9b0e-afc598c9f36b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the file read mode\n",
    "with open('data/station.csv','r') as csvfile:\n",
    "    # Create a reader object\n",
    "    station_reader = csv.reader(csvfile)\n",
    "\n",
    "    # Iterate through the reader and print out each row that is read\n",
    "    for station_row in station_reader:\n",
    "        print(station_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b3fbc2-c445-4565-b802-5276dc04da16",
   "metadata": {},
   "source": [
    "We see that each row is read into a list containing the individual column values in that row. Notice that the header is no different than any other row - it's just a list. Also notice that there is no \"container\" data structure that is preserving all these lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8432ea-47fa-47b5-a186-2694e4dcf797",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1629074d-015b-428f-81a0-f68f4bf3fd3a",
   "metadata": {},
   "source": [
    "Here are some questions to answer as a review of basic Python concepts:\n",
    "\n",
    "**Q**: What is `open`?\n",
    "\n",
    "**Q**: What are the arguments passed into `open` and what are they specifying?\n",
    "\n",
    "**Q**: What is `csvfile` and what is its data type?\n",
    "\n",
    "**Q**: What type of thing is returned by `open`?\n",
    "\n",
    "**Q**: What is the difference between `station_reader` and `csv.reader`?\n",
    "\n",
    "**Q**: Would the following line of code be ok or would it cause an error immediately when it's executed?\n",
    "\n",
    "    peanut_butter = csv.reader(csvfile)\n",
    "    \n",
    "Remember, the general form of a `for` loop block is:\n",
    "\n",
    "    for variable in <iterable>:\n",
    "        do one or more things with variable\n",
    "\n",
    "An *iterable* is a collection of objects that we can iterate over (or step through).\n",
    "\n",
    "So, in this case:\n",
    "\n",
    "* variable: `station_row`\n",
    "* collection or iterable: `station_reader`\n",
    "* something: `print(station_row)`\n",
    "\n",
    "**Q**: What kind of data is `station_row`?\n",
    "\n",
    "**Q**: If `reader` is a collection, what's it a collection of?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeb3fd1-a105-4c42-9d98-f5478c1620e7",
   "metadata": {},
   "source": [
    "So, if we want to store these lists (rows) for further processing, we need to do it ourselves. For example, we might store this data as a list of lists. Also, let's do a little data processing as we go:\n",
    "\n",
    "- only keep the `station_id`, `lat`, `long`, and `current_dockcount` fields\n",
    "- don't keep any stations that have been decomissioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c3a95-c013-43ce-ba0a-8c0a3361130e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# Create empty list to serve as container\n",
    "stations = []\n",
    "\n",
    "# Open the file read mode\n",
    "with open('data/station.csv','r') as csvfile:\n",
    "    # Create a reader object\n",
    "    station_reader = csv.reader(csvfile)\n",
    "\n",
    "    # Iterate through the reader \n",
    "    for station_row in station_reader:\n",
    "        # Check for decomissioning\n",
    "        if len(station_row[-1].strip()) == 0:\n",
    "            # Just grab the columns we want\n",
    "            new_station_row = [station_row[i] for i in [0, 2, 3, -2]]\n",
    "            # Append list (row) to container list\n",
    "            stations.append(new_station_row)\n",
    "\n",
    "# pretty print the stations list\n",
    "pprint(stations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e249ea3c-dacb-4bbc-9f4c-f471cbcfe074",
   "metadata": {},
   "source": [
    "Now the station information is stored as a big list of lists and we've only kept the columns we are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f05772-4719-41c4-89e7-4ca6e35daff3",
   "metadata": {},
   "source": [
    "### Writing a CSV file with `csv`\n",
    "\n",
    "Now let's write this list of lists back out to a new CSV file. For this we use a `writer` object along with its `writerows` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3ee1f-0f03-409e-9991-3e154eeb3120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a new file in write mode\n",
    "with open('data/station_location.csv','w') as csvfile:\n",
    "    # Create a writer object\n",
    "    station_writer = csv.writer(csvfile)\n",
    "    # Write out all the lists (rows) in the stations list (the collection or iterable)\n",
    "    station_writer.writerows(stations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803620e5-eea8-40cb-a41c-7bfdc1833e64",
   "metadata": {},
   "source": [
    "## Answers to review questions\n",
    "\n",
    "Here are some questions to answer as a review of basic Python concepts:\n",
    "\n",
    "**Q**: What is `open`?\n",
    "\n",
    "`open` is a function used to open files.\n",
    "\n",
    "**Q**: What are the arguments passed into `open` and what are they specifying?\n",
    "\n",
    "The first argument, `'data/station.csv'`, is a string specifiying the location of the file to be opened.\n",
    "The second argument, `'r'`, specifies the *mode* in which the file is opened. 'r' stands for read mode.\n",
    "\n",
    "**Q**: What is `csvfile` and what is its data type?\n",
    "\n",
    "It is a variable who type is a *file object*. Specifically, it's ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5be90c-9222-4b94-93fa-248801d445c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(csvfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2262d7e4-9248-47a2-ac88-8ec2d7459670",
   "metadata": {},
   "source": [
    "**Q**: What type of thing is returned by `open`?\n",
    "\n",
    "We just answered that, it's a file object.\n",
    "\n",
    "**Q**: What is the difference between `station_reader` and `csv.reader`?\n",
    "\n",
    "`station_reader` is a variable we created to store the instance of a `csv.reader` object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac17abd-eea3-4597-966f-d1691f5c2def",
   "metadata": {},
   "source": [
    "**Q**: Would the following line of code be ok or would it cause an error immediately when it's executed?\n",
    "\n",
    "    peanut_butter = csv.reader(csvfile)\n",
    "\n",
    "It would be perfectly fine. It's a strange name for a variable, but so be it.\n",
    "\n",
    "Remember, the general form of a `for` loop block is:\n",
    "\n",
    "    for variable in <iterable>:\n",
    "        do one or more things with variable\n",
    "\n",
    "An *iterable* is a collection of objects that we can iterate over (or step through).\n",
    "\n",
    "So, in this case:\n",
    "\n",
    "* variable: `station_row`\n",
    "* collection or iterable: `station_reader`\n",
    "* something: `print(station_row)`\n",
    "\n",
    "**Q**: What kind of data is `station_row`?\n",
    "\n",
    "It's a list containing the values in one row of the data file.\n",
    "\n",
    "**Q**: If `station_reader` is a collection, what's it a collection of?\n",
    "\n",
    "It's a collection of lists, each of which is one row of the CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4352f85-9349-45fd-b816-83146777bd83",
   "metadata": {},
   "source": [
    "### Read data into dictionaries instead of lists\n",
    "As we saw above, the `csv.reader` returns each line of the file as a list. Sometimes we want that but sometimes we might want each line to go into a different type of data structure. Instead of lists, let's read each line into a dictionary. Thankfully, the `csv` library has a `DictReader` function that does exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef16928-2f51-4ff0-a8c4-f8bc66e8b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csvfile = open('data/data-text.csv','r')\n",
    "reader = csv.DictReader(csvfile)\n",
    "\n",
    "for row in reader:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735b486-31cf-4aab-b17c-a8cf90b92bab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the file read mode\n",
    "with open('data/station.csv','r') as csvfile:\n",
    "    # Create a reader object\n",
    "    station_reader = csv.DictReader(csvfile)\n",
    "\n",
    "    # Iterate through the reader and print out each row that is read\n",
    "    for station_row in station_reader:\n",
    "        print(station_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa349c2-7a1b-4588-889e-35c57bc81eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(station_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78bcd0-98e6-49f1-8687-1d2aa4ce344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in station_row.items():\n",
    "    print(key, \" = \", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef1c2ca-2585-432a-a9dd-09ef4b129574",
   "metadata": {},
   "source": [
    "Compare the output of this program with the original version. Obviously, one prints lists and this one prints dictionaries. However, what is one advantage of this dictionary based version?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02528ec0-cba3-4bdf-bffa-1f2744f5ce55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## The bottom line\n",
    "We've learned how to use Python's built in `csv` library to read CSV files into common data structures such as lists and dictionaries. This is a common precursor to doing data cleanup and other data preparation tasks before moving on to data analysis. Again, we'll see that most Python data analysis packages have their own CSV reader functions.  \n",
    "\n",
    "Programming takes practice. Problem solving takes practice. Both Jupyter notebooks and PyCharm provide great environments for practicing your Python programming."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aap]",
   "language": "python",
   "name": "conda-env-aap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
